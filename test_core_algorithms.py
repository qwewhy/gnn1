#!/usr/bin/env python3
"""
æ ¸å¿ƒç®—æ³•ç‹¬ç«‹æµ‹è¯• - æ— éœ€æ·±åº¦å­¦ä¹ ä¾èµ–
Core algorithms standalone test - without deep learning dependencies
"""

import sys
import os
import sqlite3
from pathlib import Path

def test_database_schema():
    """æµ‹è¯•æ–°çš„æ•°æ®åº“æ¨¡å¼"""
    print("=== æµ‹è¯•æ•°æ®åº“æ¨¡å¼ ===")
    
    try:
        test_db_path = Path("test_simple.db")
        
        # æ¸…ç†æ—§çš„æµ‹è¯•æ•°æ®åº“
        if test_db_path.exists():
            test_db_path.unlink()
        
        # åˆ›å»ºæ–°æ•°æ®åº“
        conn = sqlite3.connect(test_db_path)
        cursor = conn.cursor()
        
        # ä½¿ç”¨æˆ‘ä»¬æ–°çš„æ•°æ®åº“æ¨¡å¼
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS patterns (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            edgebreaker_encoding TEXT NOT NULL,
            canonical_form TEXT NOT NULL,
            sides INTEGER NOT NULL,
            complexity_score REAL DEFAULT 0.0,
            num_vertices INTEGER DEFAULT 0,
            num_faces INTEGER DEFAULT 0,
            source_obj TEXT NOT NULL,
            quality TEXT NOT NULL,
            UNIQUE(canonical_form, sides)
        );
        """)
        
        # æ£€æŸ¥è¡¨ç»“æ„
        cursor.execute("PRAGMA table_info(patterns)")
        columns = cursor.fetchall()
        
        expected_columns = [
            'edgebreaker_encoding', 'canonical_form', 'sides', 
            'complexity_score', 'num_vertices', 'num_faces'
        ]
        
        column_names = [col[1] for col in columns]
        
        for expected in expected_columns:
            if expected in column_names:
                print(f"âœ“ æ•°æ®åº“åŒ…å«åˆ—: {expected}")
            else:
                print(f"âœ— æ•°æ®åº“ç¼ºå°‘åˆ—: {expected}")
                return False
        
        # æµ‹è¯•æ’å…¥æ–°æ ¼å¼æ•°æ®
        test_data = [
            ("2 1 3#SCLRLCRE", "2_1_3_sclrlcre", 8, 1.5, 12, 8, "test.obj", "new"),
            ("1 2#SLCR", "1_2_slcr", 6, 1.2, 10, 6, "test2.obj", "new"),
        ]
        
        for data in test_data:
            cursor.execute("""
            INSERT INTO patterns (
                edgebreaker_encoding, canonical_form, sides, 
                complexity_score, num_vertices, num_faces,
                source_obj, quality
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?);
            """, data)
        
        # éªŒè¯æ•°æ®æ’å…¥
        cursor.execute("SELECT COUNT(*) FROM patterns")
        count = cursor.fetchone()[0]
        
        if count == 2:
            print(f"âœ“ æˆåŠŸæ’å…¥ {count} æ¡æµ‹è¯•æ•°æ®")
        else:
            print(f"âœ— æ’å…¥æ•°æ®å¤±è´¥ï¼Œé¢„æœŸ2æ¡ï¼Œå®é™…{count}æ¡")
            return False
        
        # æµ‹è¯•æŸ¥è¯¢æ–°æ ¼å¼
        cursor.execute("SELECT edgebreaker_encoding, canonical_form, complexity_score FROM patterns WHERE quality='new'")
        results = cursor.fetchall()
        
        print("âœ“ æ–°æ ¼å¼æ•°æ®æŸ¥è¯¢ç»“æœ:")
        for result in results:
            print(f"  ç¼–ç : {result[0]}, è§„èŒƒå½¢å¼: {result[1]}, å¤æ‚åº¦: {result[2]}")
        
        conn.close()
        test_db_path.unlink()  # æ¸…ç†
        print("âœ“ æ•°æ®åº“æ¨¡å¼æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âœ— æ•°æ®åº“æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_encoding_format():
    """æµ‹è¯•ç¼–ç æ ¼å¼è§£æ"""
    print("\n=== æµ‹è¯•ç¼–ç æ ¼å¼ ===")
    
    try:
        # æ¨¡æ‹Ÿæˆ‘ä»¬çš„æ–°ç¼–ç æ ¼å¼
        test_encodings = [
            "2 1 3#SCLRLCRE",     # å¤šå¼¦ä¿¡æ¯ + Edgebreakeråºåˆ—
            "1 2#SLCR",           # ç®€å•æƒ…å†µ
            "#SCLR",              # ä»…Edgebreakeråºåˆ—
            "3 1 2 4#SCLRLCRSE",  # å¤æ‚å¤šå¼¦
        ]
        
        for encoding in test_encodings:
            print(f"æµ‹è¯•ç¼–ç : {encoding}")
            
            # è§£ææ ¼å¼
            if '#' in encoding:
                multi_chord_part, edgebreaker_part = encoding.split('#', 1)
            else:
                multi_chord_part = ""
                edgebreaker_part = encoding
            
            # è§£æå¤šå¼¦ä¿¡æ¯
            multi_chord_info = []
            if multi_chord_part.strip():
                for item in multi_chord_part.strip().split():
                    if item.isdigit():
                        multi_chord_info.append(int(item))
            
            # è§£æEdgebreakeræ“ä½œ
            edgebreaker_ops = []
            for char in edgebreaker_part:
                if char in ['C', 'L', 'R', 'S', 'E']:
                    edgebreaker_ops.append(char)
            
            print(f"  å¤šå¼¦ä¿¡æ¯: {multi_chord_info}")
            print(f"  Edgebreakeræ“ä½œ: {edgebreaker_ops}")
            
            # éªŒè¯è§£æç»“æœ
            if len(edgebreaker_ops) > 0:
                print(f"  âœ“ è§£ææˆåŠŸ")
            else:
                print(f"  âœ— è§£æå¤±è´¥")
                return False
        
        print("âœ“ ç¼–ç æ ¼å¼æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âœ— ç¼–ç æ ¼å¼æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_feature_dimensions():
    """æµ‹è¯•ç‰¹å¾ç»´åº¦è®¾è®¡"""
    print("\n=== æµ‹è¯•ç‰¹å¾ç»´åº¦è®¾è®¡ ===")
    
    try:
        # æ¨¡æ‹ŸPatternç‰¹å¾ (6ç»´æ‹“æ‰‘)
        pattern_features = {
            'valence': 1,           # èŠ‚ç‚¹åº¦æ•°
            'is_boundary': 1,       # è¾¹ç•Œæ ‡è®°  
            'is_corner': 1,         # è§’ç‚¹æ ‡è®°
            'distance_to_singular': 1, # åˆ°å¥‡å¼‚ç‚¹è·ç¦»
            'local_topology_config': 1, # å±€éƒ¨æ‹“æ‰‘é…ç½®
            'boundary_position_encoding': 1, # è¾¹ç•Œä½ç½®ç¼–ç 
        }
        
        # æ¨¡æ‹ŸAnchorç‰¹å¾ (6ç»´æ‹“æ‰‘ + 2ç»´å‡ ä½•)
        anchor_features = {
            # å‰6ç»´ä¸Patternå¯¹åº”
            'valence': 1,
            'is_boundary': 1,
            'is_corner': 1, 
            'distance_to_singular': 1,
            'local_topology_config': 1,
            'boundary_position_encoding': 1,
            # é¢å¤–2ç»´å‡ ä½•ç‰¹å¾
            'curvature': 1,
            'length_ratio': 1,
        }
        
        pattern_dim = len(pattern_features)
        anchor_dim = len(anchor_features)
        
        print(f"âœ“ Patternç‰¹å¾ç»´åº¦: {pattern_dim}")
        print(f"âœ“ Anchorç‰¹å¾ç»´åº¦: {anchor_dim}")
        
        # æ£€æŸ¥å‰6ç»´æ˜¯å¦å¯¹åº”
        pattern_keys = list(pattern_features.keys())
        anchor_keys = list(anchor_features.keys())[:6]  # å‰6ç»´
        
        matching_features = 0
        for i in range(6):
            if pattern_keys[i] == anchor_keys[i]:
                matching_features += 1
                print(f"  âœ“ ç»´åº¦{i}: {pattern_keys[i]} <-> {anchor_keys[i]}")
            else:
                print(f"  âœ— ç»´åº¦{i}: {pattern_keys[i]} <-> {anchor_keys[i]} ä¸åŒ¹é…")
        
        if matching_features == 6:
            print(f"âœ“ å‰6ç»´ç‰¹å¾å®Œå…¨å¯¹åº”")
            anchor_extra_keys = list(anchor_features.keys())[6:]  # ä¿®æ­£ç´¢å¼•
            print(f"âœ“ Anchoré¢å¤–ç‰¹å¾: {anchor_extra_keys}")
            return True
        else:
            print(f"âœ— åªæœ‰{matching_features}/6ç»´ç‰¹å¾å¯¹åº”")
            return False
            
    except Exception as e:
        print(f"âœ— ç‰¹å¾ç»´åº¦æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_canonical_normalization():
    """æµ‹è¯•è§„èŒƒåŒ–é€»è¾‘"""
    print("\n=== æµ‹è¯•è§„èŒƒåŒ–é€»è¾‘ ===")
    
    try:
        # æµ‹è¯•ç¼–ç è§„èŒƒåŒ–
        test_cases = [
            ("2 1 3#SCLRLCRE", "2_1_3_sclrlcre"),
            ("  1 2  # SLCR  ", "1_2_slcr"),
            ("3#SCLR", "3_sclr"),
            ("#SLCRE", "slcre"),
        ]
        
        def normalize_encoding(encoding):
            """ç®€åŒ–çš„è§„èŒƒåŒ–å®ç°"""
            normalized = encoding.strip().lower()
            normalized = normalized.replace(" ", "_").replace("#", "_")
            # ç§»é™¤è¿ç»­çš„ä¸‹åˆ’çº¿
            while "__" in normalized:
                normalized = normalized.replace("__", "_")
            return normalized.strip("_")
        
        for original, expected in test_cases:
            result = normalize_encoding(original)
            if result == expected:
                print(f"âœ“ '{original}' -> '{result}'")
            else:
                print(f"âœ— '{original}' -> '{result}', é¢„æœŸ: '{expected}'")
                return False
        
        print("âœ“ è§„èŒƒåŒ–æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âœ— è§„èŒƒåŒ–æµ‹è¯•å¤±è´¥: {e}")
        return False


def main():
    """è¿è¡Œæ‰€æœ‰æ ¸å¿ƒæµ‹è¯•"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•æ ¸å¿ƒç®—æ³•ï¼ˆæ— æ·±åº¦å­¦ä¹ ä¾èµ–ï¼‰\n")
    
    tests = [
        ("æ•°æ®åº“æ¨¡å¼", test_database_schema),
        ("ç¼–ç æ ¼å¼", test_encoding_format),
        ("ç‰¹å¾ç»´åº¦", test_feature_dimensions),
        ("è§„èŒƒåŒ–é€»è¾‘", test_canonical_normalization),
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        print(f"\n{'='*50}")
        print(f"æµ‹è¯•: {test_name}")
        print('='*50)
        
        if test_func():
            passed += 1
            print(f"âœ… {test_name} æµ‹è¯•é€šè¿‡")
        else:
            print(f"âŒ {test_name} æµ‹è¯•å¤±è´¥")
    
    print(f"\n{'='*50}")
    print(f"æ ¸å¿ƒç®—æ³•æµ‹è¯•æ€»ç»“: {passed}/{total} æµ‹è¯•é€šè¿‡")
    print('='*50)
    
    if passed == total:
        print("ğŸ‰ æ ¸å¿ƒç®—æ³•æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        print("\nâœ… éªŒè¯ç»“æœ:")
        print("â€¢ æ–°æ•°æ®åº“æ¨¡å¼è®¾è®¡æ­£ç¡®")
        print("â€¢ ç¼–ç æ ¼å¼è§£ææ­£å¸¸")  
        print("â€¢ ç‰¹å¾ç»´åº¦å®Œç¾å¯¹åº”")
        print("â€¢ è§„èŒƒåŒ–é€»è¾‘å·¥ä½œæ­£å¸¸")
        
        print("\nğŸ“‹ ä¸‹ä¸€æ­¥æ“ä½œå»ºè®®:")
        print("1. å®‰è£…æ·±åº¦å­¦ä¹ ä¾èµ–: pip install torch torch-geometric trimesh")
        print("2. è¿è¡Œå®Œæ•´æµ‹è¯•: python test_new_implementation.py")
        print("3. é‡å»ºæ•°æ®åº“: python -m src.data_processing.populate_db")
        print("4. å¼€å§‹è®­ç»ƒ: python -m src.training.train")
        
        return True
    else:
        print("âš ï¸  éƒ¨åˆ†æ ¸å¿ƒæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°ã€‚")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)